import cv2
import mediapipe as mp
import numpy as np
import math
from flask import Flask
from flask_socketio import SocketIO
import threading
import time
from scipy.spatial.transform import Rotation as R

app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

REAL_HAND_WIDTH_CM = 7.0
FOCAL_LENGTH = 600.0
TARGET_Z = 30.0

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(model_complexity=0, max_num_hands=1,
                       min_detection_confidence=0.7, min_tracking_confidence=0.7)

def run_camera():
    cap = cv2.VideoCapture(0)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cx, cy = width // 2, height // 2

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        frame = cv2.flip(frame, 1)
        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                lm = hand_landmarks.landmark
                wrist_u, wrist_v = int(lm[0].x * width), int(lm[0].y * height)

                p0 = np.array([lm[0].x, lm[0].y, lm[0].z])
                p5 = np.array([lm[5].x, lm[5].y, lm[5].z])
                p17 = np.array([lm[17].x, lm[17].y, lm[17].z])

                v1 = p5 - p0
                v2 = p17 - p0
                z_axis = np.cross(v1, v2)
                z_axis /= (np.linalg.norm(z_axis) + 1e-6)
                x_axis = v1 / (np.linalg.norm(v1) + 1e-6)
                y_axis = np.cross(z_axis, x_axis)

                rot_matrix = np.stack([x_axis, y_axis, z_axis], axis=1)
                quat = R.from_matrix(rot_matrix).as_quat()

                p_width_px = math.sqrt(((lm[5].x - lm[17].x) * width) ** 2 + ((lm[5].y - lm[17].y) * height) ** 2)
                z_cam = (REAL_HAND_WIDTH_CM * FOCAL_LENGTH) / (p_width_px + 1e-6)
                pos_x = round((lm[0].x * width - cx) * z_cam / FOCAL_LENGTH, 2)
                pos_y = round(-(lm[0].y * height - cy) * z_cam / FOCAL_LENGTH, 2)
                pos_z = round(z_cam - TARGET_Z, 2)

                payload = {
                    'x': pos_x, 'y': pos_y, 'z': pos_z,
                    'qx': round(quat[0], 4), 'qy': round(quat[1], 4),
                    'qz': round(quat[2], 4), 'qw': round(quat[3], 4)
                }
                socketio.emit('hand_update', payload)

                log_msg = f"Pos: [{pos_x:>6.2f}, {pos_y:>6.2f}, {pos_z:>6.2f}] | Quat: [{quat[0]:>5.2f}, {quat[1]:>5.2f}, {quat[2]:>5.2f}, {quat[3]:>5.2f}]"
                print(log_msg)

                cv2.circle(frame, (wrist_u, wrist_v), 8, (0, 255, 0), -1)
                info_text = f"XYZ: {pos_x}, {pos_y}, {pos_z}"
                cv2.putText(frame, info_text, (wrist_u + 15, wrist_v),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        cv2.imshow('Hand Tracker 3.0', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'): break
        time.sleep(0.01)

    cap.release()
    cv2.destroyAllWindows()

if __name__ == '__main__':
    threading.Thread(target=run_camera, daemon=True).start()
    socketio.run(app, host='127.0.0.1', port=5001, allow_unsafe_werkzeug=True)
